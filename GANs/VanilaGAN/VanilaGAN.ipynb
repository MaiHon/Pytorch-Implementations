{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from visdom import Visdom\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "viz = Visdom()\n",
    "viz.close(env='main')\n",
    "\n",
    "def loss_tracker(loss_plot, loss_value, num):\n",
    "    viz.line(X=num, Y=loss_value, win=loss_plot, update='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(nn.Module):\n",
    "    def __init__(self, noise=128):\n",
    "        super(G, self).__init__()\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.G = torch.nn.Sequential(\n",
    "            torch.nn.Linear(noise, 256),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Linear(256, 28*28),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.G(x)\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.D = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28*28, 256),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Linear(256, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.D(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "Layer(type)     ||        Kernel Shape         Output Shape         Param #\n",
      "============================================================================\n",
      "G Inputs        ||                   -            [16, 128]               -\n",
      "                ||                                                         \n",
      "1> G-G-Linear   ||          [128, 256]            [16, 256]          33,024\n",
      "2> G-G-ReLU     ||                   -            [16, 256]               0\n",
      "3> G-G-Linear   ||          [256, 784]            [16, 784]         201,488\n",
      "4> G-G-Sigmoid  ||                   -            [16, 784]               0\n",
      "============================================================================\n",
      "Total params: 234,512\n",
      "Trainable params: 234,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 0.25\n",
      "Params size (MB): 0.89\n",
      "Estimated Total Size (MB): 1.27\n",
      "----------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "Layer(type)     ||        Kernel Shape         Output Shape         Param #\n",
      "============================================================================\n",
      "D Inputs        ||                   -            [16, 784]               -\n",
      "                ||                                                         \n",
      "1> D-D-Linear   ||          [784, 256]            [16, 256]         200,960\n",
      "2> D-D-ReLU     ||                   -            [16, 256]               0\n",
      "3> D-D-Linear   ||            [256, 1]              [16, 1]             257\n",
      "4> D-D-Sigmoid  ||                   -              [16, 1]               0\n",
      "============================================================================\n",
      "Total params: 201,217\n",
      "Trainable params: 201,217\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------\n",
      "Input size (MB): 0.77\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.77\n",
      "Estimated Total Size (MB): 1.60\n",
      "----------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device == torch.device('cuda'):\n",
    "    torch.backends.cudnn.benchmark=True\n",
    "    \n",
    "Gen = G().to(device)\n",
    "Dis = D().to(device)\n",
    "\n",
    "# from torchsummary import summary\n",
    "# from torchsummaryX import summary as summaryX\n",
    "from torchsummaryM import summary\n",
    "summary(Gen, (16, 128), device=device)\n",
    "summary(Dis, (16, 28*28), device=device)\n",
    "# print(\"=\"*70)\n",
    "# summaryX(Gen, torch.zeros((100, 128)))\n",
    "# summaryX(Dis, torch.zeros((100, 28*28)))\n",
    "# print(\"=\"*70)\n",
    "# summaryM(Gen, (128, ), batch_size=100, device=device)\n",
    "# summaryM(Dis, (28*28, ), batch_size=100, device=device)\n",
    "\n",
    "\n",
    "g_optimizer = torch.optim.Adam(Gen.parameters(), 2e-4)\n",
    "d_optimizer = torch.optim.Adam(Dis.parameters(), 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngpu = torch.cuda.device_count()\n",
    "\n",
    "# if (device == 'cuda') and (ngpu > 1):\n",
    "#     Dis = nn.DataParallel(Dis, list(range(ngpu)))\n",
    "\n",
    "# if (device == 'cuda') and (ngpu > 1):\n",
    "#     Gen = nn.DataParallel(Gen, list(range(ngpu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset Prepare\n",
    "dataset = dsets.MNIST('./MNIST', \n",
    "                    train=True, \n",
    "                    transform=transforms.ToTensor(),\n",
    "                    target_transform=None,\n",
    "                    download=True)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = dsets.ImageFolder(root=\"/Users/honeyeob/python_workspace/workspace/Pytorch/GAN/img_align_celeba\",\n",
    "#                 transform=transforms.Compose([\n",
    "#                     transforms.Grayscale(),\n",
    "#                     transforms.Resize((28, 28)),\n",
    "#                     transforms.ToTensor()\n",
    "#                 ])\n",
    "# )\n",
    "# data_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Noise for Testing\n",
    "test_noise = Variable(torch.randn(25, 128)).to(device)\n",
    "image_window = viz.images(torch.randn(25, 1, 28, 28), \n",
    "                        opts=dict(title = \"Generated Imgs\",\n",
    "                        caption = \"Generated Image-{}-{}\".format(0, 0)))\n",
    "loss_plt = viz.line(Y=torch.randn(1, 2).zero_(), \n",
    "                    opts=dict(title='Tracking Losses',\n",
    "                    legend=['D_Loss', 'G_aLoss'], \n",
    "                    showlegend=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [104/200], Step[20/118], d_loss: 0.2935, g_loss: 3.1936\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2e3b58b1b835>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdis_real_results\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdis_fake_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mDis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0md_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0md_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\h\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\h\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_step = 0\n",
    "total_batch = len(data_loader)\n",
    "\n",
    "for epoch in range(200):\n",
    "    for step, data in enumerate(data_loader):\n",
    "        images = data[0]\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Train D\n",
    "        noise = Variable(torch.randn(images.size(0), 128))\n",
    "        noise = noise.to(device)\n",
    "        fake_images = Gen(noise)\n",
    "        dis_fake_results = Dis(fake_images)\n",
    "        dis_real_results = Dis(images.reshape(-1, np.prod(images.shape[1:])))\n",
    "\n",
    "        d_loss = -torch.mean(torch.log(dis_real_results) + torch.log(1-dis_fake_results))\n",
    "        Dis.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "\n",
    "        # Train G\n",
    "        noise = Variable(torch.randn(images.size(0), 128))\n",
    "        noise = noise.to(device)\n",
    "        fake_images = Gen(noise)\n",
    "        dis_fake_results = Dis(fake_images)\n",
    "        D_G_z  = dis_fake_results\n",
    "        g_loss = - torch.mean(torch.log(dis_fake_results) + 1e-6)\n",
    "\n",
    "        Gen.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        total_step += 1\n",
    "        \n",
    "        # Print & Showing via Visdom\n",
    "        if (step + 1) % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, g_loss: %.4f'% (epoch + 1, 200, step + 1, total_batch, d_loss.data, g_loss.data))\n",
    "            fake_images = Gen(test_noise)\n",
    "            fake_images = fake_images.reshape(25, 1, 28, 28)\n",
    "            loss_tracker(loss_plt, np.column_stack((d_loss.detach().cpu().data, g_loss.detach().cpu().data)), \n",
    "                        np.column_stack((torch.Tensor([total_step]), \n",
    "                                         torch.Tensor([total_step]))\n",
    "                        )\n",
    "            )\n",
    "            \n",
    "            image_window = viz.images(fake_images.data,\n",
    "                                    opts=dict(title = \"Generated Imgs\",\n",
    "                                    caption = \"Generated Image-{}-{}\".format(epoch + 1, step + 1)),\n",
    "                                    win = image_window\n",
    "            )\n",
    "        \n",
    "        # Image Save\n",
    "        if (epoch + 1) % 10 == 0 and (step+1) == total_batch:\n",
    "            fake_images = Gen(test_noise)\n",
    "            fake_images = fake_images.reshape(25, 1, 28, 28).detach()\n",
    "            save_image(fake_images.data, './IMGS/generatedimage-%d-%d.png' % (epoch + 1, step + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = 0\n",
    "total_batch = len(data_loader)\n",
    "\n",
    "for epoch in range(200):\n",
    "    epoch_average_g_loss = 0\n",
    "    epoch_average_d_loss = 0\n",
    "    \n",
    "    for step, data in enumerate(data_loader):\n",
    "        images = data[0]\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Train D\n",
    "        noise = Variable(torch.randn(images.size(0), 128))\n",
    "        noise = noise.to(device)\n",
    "        fake_images = Gen(noise)\n",
    "        dis_fake_results = Dis(fake_images)\n",
    "        dis_real_results = Dis(images.reshape(-1, np.prod(images.shape[1:])))\n",
    "\n",
    "        d_loss = -torch.mean(torch.log(dis_real_results) + torch.log(1-dis_fake_results))\n",
    "        Dis.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train G\n",
    "        noise = Variable(torch.randn(images.size(0), 128))\n",
    "        noise = noise.to(device)\n",
    "        fake_images = Gen(noise)\n",
    "        dis_fake_results = Dis(fake_images)\n",
    "        g_loss = - torch.mean(torch.log(dis_fake_results) + 1e-6)\n",
    "\n",
    "        Gen.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        total_step += 1\n",
    "        \n",
    "        epoch_average_d_loss += d_loss.detach().cpu().data / total_batch\n",
    "        epoch_average_g_loss += g_loss.detach().cpu().data / total_batch\n",
    "        \n",
    "        # Print & Showing via Visdom\n",
    "        if (step + 1) % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, g_loss: %.4f'% (epoch + 1, 200, step + 1, total_batch, d_loss.data, g_loss.data))\n",
    "            fake_images = Gen(test_noise)\n",
    "            fake_images = fake_images.reshape(10, 1, 28, 28)\n",
    "            image_window = viz.images(fake_images.data,\n",
    "                                    opts=dict(title = \"Generated Imgs\",\n",
    "                                    caption = \"Generated Image-{}-{}\".format(epoch + 1, step + 1)),\n",
    "                                    win = image_window\n",
    "            )\n",
    "        \n",
    "        # Image Save\n",
    "        if (epoch + 1) % 10 == 0 and (step+1) == total_batch:\n",
    "            fake_images = Gen(test_noise)\n",
    "            fake_images = fake_images.reshape(10, 1, 28, 28).detach()\n",
    "            save_image(fake_images.data, './IMGS/generatedimage-%d-%d.png' % (epoch + 1, step + 1))\n",
    "            \n",
    "    loss_tracker(loss_plt, np.column_stack((epoch_average_d_loss, epoch_average_g_loss)), \n",
    "                        np.column_stack((torch.Tensor([epoch]), \n",
    "                        torch.Tensor([epoch]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
